{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embeddings - Keras 101.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gMo8fyFIENE",
        "colab_type": "text"
      },
      "source": [
        "## Word Embeddings\n",
        "\n",
        "Use IMDB movie review dataset and create word embeddings that are essential for any task involving text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BqMGOfMINcH",
        "colab_type": "code",
        "outputId": "eb559194-f06f-4280-eeac-e611abf65eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,Flatten,Dense  \n",
        "from keras import preprocessing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evJIYoFT_7bL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Gain access to Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22KhZB0SFlQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 10000  #No of words to consider as features\n",
        "maxlen = 20     # max length of a sample\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=maxlen) #turns list of integers into a 2D integer tensor of shape (samples,maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g6Whet0JZjR",
        "colab_type": "text"
      },
      "source": [
        "Using an Embedding layer and classifier on the IMDB data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWVVKL6KJaey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000,8,input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train,y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tz76uvSPu1s",
        "colab_type": "text"
      },
      "source": [
        "By looking at only the first 20 words and training a single Dense layer on top, we still get a validation accuracy of ~76%.  \n",
        "We can improve this further by using pretrained word embeddings alongwith recurrent layers of 1D conv layers to learn features that take into account each sequence as a whole."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7nh9AwOPyF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load raw IMDB text\n",
        "imdb_dir = './gdrive/My Drive/Colab Notebooks/data/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir,'train')\n",
        "\n",
        "labels, texts = [],[]\n",
        "\n",
        "for label_type in ['neg','pos']:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == '.txt':\n",
        "      f = open(os.path.join(dir_name,fname))\n",
        "      texts.append(f.read())\n",
        "      f.close()\n",
        "      \n",
        "      if label_type=='neg':\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)\n",
        "\n",
        "len(texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg6GfcshFIKv",
        "colab_type": "text"
      },
      "source": [
        "Tokenizing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFSNRlhOFMcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 100  #cut of reviews after 100 words\n",
        "training_samples, validation_samples = 200, 1000 #small training samples since we use pretrained embeddings    \n",
        "max_words = 10000  #consider top 10k words in dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)  #convert from words to integer vectors\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen) \n",
        "\n",
        "labels = np.asarray(labels)\n",
        "\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])  #Split data into training and validation after shuffling it because samples are ordered \n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples : training_samples+validation_samples]\n",
        "y_val = labels[training_samples : training_samples+validation_samples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0RCh7qYq0a7",
        "colab_type": "text"
      },
      "source": [
        "Parse the GloVe word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2CQXtcorEAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_dir = './gdrive/My Drive/Colab Notebooks/data/glove.6B'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMudV2yP1mDr",
        "colab_type": "text"
      },
      "source": [
        "Build an embedding matrix to load into an Embedding layer. Matrix shape (max_words, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtpx1rCe1mkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((max_words,embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "  if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul0dOgnDZQdE",
        "colab_type": "text"
      },
      "source": [
        "Define the model - similar as the one we used before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhJoAty2ZOYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#Load the pretrained embeddings into the embedding layer\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZCPx2x5aL27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_train,y_train,\n",
        "                   epochs=10,\n",
        "                   batch_size=32,\n",
        "                   validation_data=(x_val,y_val))\n",
        "\n",
        "model.save('gdrive/My Drive/Colab Notebooks/models/pretrained_glove_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVPDcNGRd7eI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1,len(loss)+1)\n",
        "\n",
        "plt.plot(epochs,acc,'bo',label='Training accuracy')\n",
        "plt.plot(epochs,val_acc,'b',label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGZIrKPWezyf",
        "colab_type": "text"
      },
      "source": [
        "The model quickly overfits since we used limited training samples.  \n",
        "To improve model performance-\n",
        "1) Skip using pretrained embeddings and increase training sample size to learn task specific embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM3dQuQJffZW",
        "colab_type": "text"
      },
      "source": [
        "Tokenize test data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3HLVfgXfZGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dir = os.path.join(imdb_dir,'test')\n",
        "\n",
        "labels_test, texts_test = [], []\n",
        "\n",
        "for label_type in ['pos','neg']:\n",
        "  dir_name = os.path.join(test_dir,label_type)\n",
        "  for fname in sorted(os.listdir(dir_name)):\n",
        "    if fname[-4:]=='.txt':\n",
        "      f = open(os.path.join(dir_name,fname))\n",
        "      texts_test.append(f.read())\n",
        "      f.close()\n",
        "      \n",
        "      if label_type == 'neg':\n",
        "        labels_test.append(0)\n",
        "      else:\n",
        "        labels_test.append(1)\n",
        "        \n",
        "sequences = tokenizer.texts_to_sequences(texts_test)\n",
        "x_test = pad_sequences(sequences,maxlen=maxlen)\n",
        "y_test = np.asarray(labels_test)\n",
        "\n",
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}